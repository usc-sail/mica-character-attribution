story + label-dependent

for component g in components G:
    M_g is the set of movies in g
    C_g is the set of characters in g
    T_g is the set of traits in g
    calculate the label matrix L of size |C_g| x |T_g|
    initialize the character embedding matrix X of size |C_g| x |T_g| x |M_g| x d
    for movie m in g:
        C_m is the set of characters in m
        T_m is the set of traits in m
        run the model on movie m and traits T_g
        model returns a character embedding matrix X_m of size |C_m| x |T_m| x d
        update X with X_m at the proper index
    run classifier C on X to get probability matrix P of size |C_g| x |T_g| x |M_g|
    P-mil = max(P, axis = movie axis) so P-mil is of size |C_g| x |T_g|
    get cross-entropy loss between L and P-mil
    backpropagate loss

classifier C (inp of size * x d)
    apply FFN to get matrix of size * x 1
    use sigmoid to get probability


story + label-independent

for component g in components G:
    M_g is the set of movies in g
    C_g is the set of characters in g
    T_g is the set of traits in g
    find the trait embedding matrix W of size |T_g| x d
    calculate the label matrix L of size |C_g| x |T_g|
    initialize the character embedding matrix X of size |C_g| x |M_g| x d
    for movie m in g:
        C_m is the set of characters in m
        run the model on movie m
        model returns a character embedding matrix X_m of size |C_m| x d
        update X with X_m at the proper index
    run classifier D on X and W to get probability matrix P of size |C_g| x |T_g| x |M_g|
    P-mil = max(P, axis = movie axis) so P-mil is of size |C_g| x |T_g|
    get cross-entropy loss between L and P-mil
    backpropagate loss

classifier D (inp1 of size * x d, inp2 of size k x d)
    concatenate to get a matrix of size * x k x d and apply FFN + sigmoid to get probability matrix of size * x k
    or you can use dot product and then apply FFN + sigmoid


character + label-dependent

for character c in characters C:
    M_c is the set of movies where c appears
    T_c is the set of positive traits and K_c is the set of negative traits
    calculate the label arr L of size |T_c| + |K_c|
    initialize the character embedding matrix X of size |M_c| x (|T_c| + |K_c|) x d
    for movie m in M_c:
        run the model on movie m and traits T_c + K_c
        model returns a character embedding X_m of size d
        update X with X_m
    run classifier C on X to get probability matrix P of size |M_c| x (|T_c| + |K_c|)
    P-mil = max(P, axis = movie axis) so P-mil is of size |T_c| + |K_c|
    get cross-entropy loss between L and P-mil
    backpropagate


character + label-independent

for character c in characters C:
    M_c is the set of movies where c appears
    T_c is the set of positive traits and K_c is the set of negative traits
    find the trait embedding matrix W of size (|T_c| + |K_c|) x d
    calculate the label arr L of size |T_c| + |K_c|
    initialize the character embedding matrix X of size |M_c| x d
    for movie m in M_c:
        run the model on movie m
        model returns a character embedding X_m of size d
        update X with X_m
    run classifier D on X and W to get probability matrix P of size (|T_c| + |K_c|) x |M_c|
    P-mil = max(P, axis = movie axis) so P-mil is of size |T_c| + |K_c|
    get cross-entropy loss between L and P-mil
    backpropagate

training:
for movie m in movies M:
    C is the set of characters that appear in movie m
    T is the set of tropes portrayed by characters C in the partition P
    Batch T into subsets each of size s
    For each subbatch t of T:
        run model on movie m
        model returns C x t x d matrix
        run classifier to get C x t logits
        create labels L of shape C x t
        assumption: 1) if C portrays trope, label is 1, 2) 
